{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5695d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Germany'\n",
    "city = 'Berlin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b795d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import selenium\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from housing_functions import get_driver\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pprint\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "import demjson3\n",
    "import os\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "from housing_functions import random_click, get_driver, search_for_place\n",
    "\n",
    "import json\n",
    "\n",
    "import logging\n",
    "import colorlog\n",
    "\n",
    "import sys \n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1448b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "255dbd24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98acdb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_listings(driver):\n",
    "    pass\n",
    "    \n",
    "def get_logger():\n",
    "    handler = colorlog.StreamHandler()\n",
    "    formatter = colorlog.ColoredFormatter(\n",
    "        \"%(log_color)s%(asctime)s [%(levelname)s] %(message)s\",\n",
    "        log_colors={\n",
    "            \"DEBUG\": \"cyan\",\n",
    "            \"INFO\": \"green\",\n",
    "            \"WARNING\": \"yellow\",\n",
    "            \"ERROR\": \"red\",\n",
    "            \"CRITICAL\": \"bold_red\",\n",
    "        }\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    \n",
    "    logger = colorlog.getLogger(__name__)\n",
    "    logger.addHandler(logging.FileHandler(\"logging.txt\", encoding=\"utf-8\"))\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "    \n",
    "def accept_cookies(driver):\n",
    "    cookies_button = driver.find_element(By.ID,'onetrust-accept-btn-handler')\n",
    "    cookies_button.click()\n",
    "    \n",
    "    \n",
    "def get_container_listings(container):\n",
    "    container.find_elements(By.CLASS_NAME,'css-1efwqj7-cardLink')\n",
    "    return container\n",
    "\n",
    "\n",
    "def create_directory_for_photos():\n",
    "    if not os.path.exists(f'{country}_{city}_house_photos'):\n",
    "        os.mkdir(f'{country}_{city}_house_photos')\n",
    "\n",
    "\n",
    "\n",
    "def get_geo_data(driver):\n",
    "    try:\n",
    "        latitude, longitude, number_of_rooms = None, None, None\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source,'html.parser')\n",
    "        script_elements = soup.find_all('script', type='application/ld+json')\n",
    "        for script in script_elements:\n",
    "            if script.string:\n",
    "                data = json.loads(script.string)\n",
    "                \n",
    "                if data.get('@type') != 'Accommodation':\n",
    "                    continue\n",
    "                geo_data = data.get('geo')\n",
    "                latitude = geo_data.get('latitude')\n",
    "                longitude = geo_data.get('longitude')\n",
    "                number_of_rooms = data.get('numberOfRooms')         \n",
    "    except :\n",
    "        print('Inside the geo_data function, an Exception has occured')    \n",
    "        return None, None, None\n",
    "    \n",
    "    return latitude, longitude, number_of_rooms\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd01c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_container(driver):\n",
    "    page_container = driver.find_element(By.CLASS_NAME, 'css-wp5dsn-container')\n",
    "    return page_container\n",
    "\n",
    "def get_container_listings(container):\n",
    "     rows = container.find_elements(By.CLASS_NAME,'css-1efwqj7-cardLink')\n",
    "     return rows\n",
    "\n",
    "\n",
    "def scroll_page(driver):\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "\n",
    "def scroll_down_and_wait_2_secs(driver):\n",
    "    driver.execute_script(\"window.scrollBy(0, 4000);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "# def scrape_half_page(driver):\n",
    "    \n",
    "#     listings_data_of_this_page = []\n",
    "    \n",
    "#     container = get_container(driver)\n",
    "#     container_listings = get_container_listings(container)\n",
    "    \n",
    "    \n",
    "#     for listing in container_listings:\n",
    "        \n",
    "#         listing_data =scrape_listing(listing,driver)\n",
    "#         listings_data_of_this_page.append(listing_data)\n",
    "    \n",
    "    \n",
    "#     return listings_data_of_this_page\n",
    "    \n",
    "    \n",
    "def get_listing_tags(driver):\n",
    "    try:\n",
    "        tags_container = driver.find_element(By.CLASS_NAME, 'css-q6fy6c-highlightContainer')\n",
    "        if tags_container:\n",
    "            container_elements = tags_container.find_elements(By.CLASS_NAME, 'css-1e5azn1-highlightItem')\n",
    "            tags = []\n",
    "            for tag_element in container_elements:\n",
    "                tag = tag_element.text\n",
    "                tags.append(tag)\n",
    "            return tags\n",
    "    except:\n",
    "        return [None]\n",
    "        \n",
    "        \n",
    "def scrape_listing_photos_and_create_their_file(driver, listing_id:str):\n",
    "    folder = os.path.join(f'{country}_{city}_house_photos', listing_id)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    #images_buttons = driver.find_elements(By.CLASS_NAME,'css-13emeri-tile-tileButton')\n",
    "        \n",
    "    # click on more photos so that more photos are loaded\n",
    "    more_photos_button = driver.find_element(By.CLASS_NAME,'css-13emeri-tile-tileButton')\n",
    "    more_photos_button.click()\n",
    "    images = driver.find_elements(By.TAG_NAME, 'img')   \n",
    "    \n",
    "    unique_images = set(images)\n",
    "    \n",
    "    zip_file_path = os.path.join(folder, f\"{listing_id}_photos.zip\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_file_path, 'w', compression=zipfile.ZIP_DEFLATED) as f1:\n",
    "        \n",
    "        for i, image in enumerate(unique_images):\n",
    "            size = image.size\n",
    "            height = size['height']\n",
    "            if height >= 45:\n",
    "                image_url = image.get_attribute('src') or image.get_attribute('data-src') or image.get_attribute('data-lazy') or image.get_attribute('data-original')\n",
    "                \n",
    "                if image_url.startswith(\"http\"):\n",
    "\n",
    "                    response = requests.get(image_url)\n",
    "                    image_to_be_written = response.content\n",
    "                    \n",
    "                    image_bytes = len(image_to_be_written)\n",
    "                    \n",
    "                    if image_bytes > 100 * 1000:\n",
    "\n",
    "                        image_name = listing_id + '_' + str(i) + '.jpg'\n",
    "                        f1.writestr(image_name, image_to_be_written)\n",
    "\n",
    "    \n",
    "                else:\n",
    "                    print(f'image {i} doesnt start with http')\n",
    "                    print(image_url)\n",
    "\n",
    "    \n",
    "    \n",
    "def create_metadata_file_in_the_listings_folder(listing_id,title,price,area,description,full_description,tags,latitude,longitude,number_of_rooms):\n",
    "    try:\n",
    "        print(listing_id.__class__)\n",
    "        if not isinstance(listing_id,str):\n",
    "            logger.error(f'Listing with id; {listing_id} is not of type; str')\n",
    "            raise ValueError\n",
    "        folder = os.path.join(f'{country}_{city}_house_photos', listing_id)    \n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        logger.info(f'Created folder or folder already exists for listing with id; {listing_id}')\n",
    "        file = os.path.join(folder,'.metadata')\n",
    "        with open (file, 'w', encoding='utf-8') as f1:\n",
    "            meta_data = {\n",
    "                'listing_id' : listing_id,\n",
    "                'title' : title,\n",
    "                'price' : price,\n",
    "                'area' : area,\n",
    "                'description' : description,\n",
    "                'full_description' : full_description,\n",
    "                'latitude' : latitude,\n",
    "                'longitude'  : longitude,\n",
    "                'number_of_rooms' : number_of_rooms\n",
    "            }\n",
    "            for i, tag in enumerate(tags):\n",
    "                key = f'tag_{i}'\n",
    "                meta_data[key] = tag\n",
    "                \n",
    "            json_meta_data = json.dumps(meta_data, ensure_ascii= False, indent=4)\n",
    "            f1.write(json_meta_data)\n",
    "            logger.info(f'Wrote meta_data file for listing with id; {listing_id}')\n",
    "    except Exception :\n",
    "        logger.error(f'Encountered error during the writing of the Meta Data file for listing with id; {listing_id}')\n",
    "        driver.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def get_listing_text_attributes(driver):\n",
    "    try:\n",
    "        title = driver.find_element(By.CLASS_NAME,'css-1ql5bbl').text\n",
    "        price = driver.find_element(By.CLASS_NAME, 'css-1bop1zx-pricingContent').text\n",
    "        area = driver.find_element(By.CLASS_NAME, 'css-2ccjfp').text\n",
    "        description = driver.find_element(By.CLASS_NAME, 'css-31lj5q').text\n",
    "        full_description = driver.find_element(By.CLASS_NAME, 'css-1liw7jd-preWrap-breakWord').text\n",
    "\n",
    "        return title, price, area, description, full_description\n",
    "    \n",
    "    except Exception:\n",
    "        return (None,None,None,None,None,)\n",
    "        \n",
    "\n",
    "\n",
    "def get_listing_id(driver):\n",
    "    current_url = driver.current_url\n",
    "    if '/ut' in current_url:\n",
    "        listing_id = current_url.split('/ut')[1].split('/de')[0]\n",
    "    else:    \n",
    "        listing_id = current_url.split('-')[-1]\n",
    "    return listing_id\n",
    "\n",
    "\n",
    "\n",
    "def scrape_listing(listing, driver, original_window):\n",
    "    try:\n",
    "        listing.click()\n",
    "    except Exception:\n",
    "        logger.error('Didn t manage to sclick on listing.')\n",
    "    \n",
    "    try:\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        listing_id = get_listing_id(driver)\n",
    "        logger.info(f'Acquired listing id {listing_id}')\n",
    "        \n",
    "        title, price, area, description, full_description = get_listing_text_attributes(driver)\n",
    "        logger.info(f'Title : {title}, price : {price}, area : {area}, description : {description}, fulldescription : {full_description}')\n",
    "        \n",
    "        tags = get_listing_tags(driver)\n",
    "        logger.info(f'Tags : {tags}' )\n",
    "        \n",
    "        scrape_listing_photos_and_create_their_file(driver,listing_id)\n",
    "        logger.info('Scraped photos')\n",
    "        \n",
    "        latitude, longitude, number_of_rooms = get_geo_data(driver)\n",
    "        logger.info(f'Latitude {latitude}, longitude : {longitude}, Number of Rooms : {number_of_rooms}')\n",
    "\n",
    "        create_metadata_file_in_the_listings_folder(listing_id,title,price,area,description,full_description,tags,latitude,longitude,number_of_rooms)\n",
    "        logger.info(f'Created metadata file for listing with id ; {listing_id}')\n",
    "    \n",
    "    except Exception:\n",
    "        logger.error('something failed when scraping the data')\n",
    "    finally:\n",
    "        \n",
    "        if driver.current_window_handle != original_window:\n",
    "            driver.close()\n",
    "            driver.switch_to.window(original_window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85857577",
   "metadata": {},
   "source": [
    "<button class=\"MuiButtonBase-root MuiButton-root MuiButton-contained MuiButton-containedPrimary MuiButton-sizeMedium MuiButton-containedSizeMedium MuiButton-fullWidth MuiButton-root MuiButton-contained MuiButton-containedPrimary MuiButton-sizeMedium MuiButton-containedSizeMedium MuiButton-fullWidth css-tkyhxc-button-button\" tabindex=\"0\" type=\"submit\" data-test-locator=\"Search and book\">Search<span class=\"MuiTouchRipple-root css-w0pj6f\"></span></button>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562f5c7",
   "metadata": {},
   "source": [
    "******** PROGRAM STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3a2c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "driver = get_driver()\n",
    "create_directory_for_photos()\n",
    "#ipython = get_ipython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fbb9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tries = 0\n",
    "while True:\n",
    "    try:\n",
    "        tries += 1\n",
    "        if tries == 2:\n",
    "            os.execv(sys.executable, ['python'] + sys.argv)\n",
    "\n",
    "        search_for_place(driver)\n",
    "        accept_cookies(driver)\n",
    "        break\n",
    "\n",
    "    except Exception :\n",
    "        logger.error('Exception while trying to search for place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ba4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_half_page(driver):\n",
    "    try:\n",
    "        logger.info('Trying to get the container element')\n",
    "        container = driver.find_element(By.CLASS_NAME, 'css-wp5dsn-container')\n",
    "        \n",
    "        scroll_page(driver)\n",
    "        logger.info('Trying to get the container\\'s listings')\n",
    "        container_listings = get_container_listings(container)\n",
    "        \n",
    "        original_window = driver.current_window_handle\n",
    "\n",
    "        logger.info('Iterating over the container\\'s listings')\n",
    "        for  listing in container_listings:\n",
    "            try:\n",
    "                logger.info('Trying to scrape a listing from the container')\n",
    "                listing_data = scrape_listing(listing, driver, original_window)\n",
    "            except ElementClickInterceptedException:\n",
    "                logger.error('Failed to scrape a listing from the container')\n",
    "    except Exception :\n",
    "        logger.error('Undefined error')\n",
    "\n",
    "    finally:\n",
    "        print('containers;', len(container_listings))\n",
    "\n",
    "def scrape_page(driver):\n",
    "    '''\n",
    "    For each page, scrape its upper half first.\n",
    "    Then scroll down a bit, to get more container listings, and scrape the other half\n",
    "    '''\n",
    "    scrape_half_page(driver)\n",
    "    driver.execute_script(\"window.scrollBy(0, 2000);\")\n",
    "    scrape_half_page(driver)\n",
    "\n",
    "\n",
    "def go_to_next_page(driver):\n",
    "    logger.info('Trying to go to the next Page')\n",
    "    next_button = driver.find_element(By.CSS_SELECTOR, \"button[aria-label='Go to next page']\")\n",
    "    next_button.click()\n",
    "    logger.info('Clicked on next Page')\n",
    "\n",
    "\n",
    "def scrape_pages(driver):\n",
    "    logger.info('Started Scraping Pages')\n",
    "    pages_scraped = 0\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                logger.info(f'Trying to Scrape the {pages_scraped}th Page')\n",
    "                scrape_page(driver)\n",
    "                logger.info(f'Successfuly scraped the {pages_scraped}th Page')\n",
    "                \n",
    "                pages_scraped += 1        \n",
    "                \n",
    "                logger.info(f'Trying to click on the {pages_scraped}th Page')\n",
    "                \n",
    "                scroll_down_and_wait_2_secs(driver)\n",
    "\n",
    "                go_to_next_page(driver)\n",
    "                \n",
    "                if pages_scraped == 100: # we do not need to scrape more than 100 pages\n",
    "                    logger.info('Scraped 100 pages, Now exiting')\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                logger.error(f'Encounterd Error during the scraping of the {pages_scraped}th Page', exc_info=True)\n",
    "    finally:\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13c14aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 17:04:40,411 [INFO] Started Scraping Pages\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:40,412 [INFO] Trying to Scrape the 0th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:40,413 [INFO] Trying to get the container element\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:41,516 [INFO] Trying to get the container's listings\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:41,535 [INFO] Iterating over the container's listings\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:41,535 [INFO] Trying to scrape a listing from the container\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:43,610 [INFO] Acquired listing id 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:43,681 [INFO] Title : Studyo, price : From\n",
      "€850 /month, area : 16 m²+, description : Cleaning in common areas, fulldescription : BOOK NOW & get the December rent for free!*\n",
      "\n",
      "Find your preferred room, book online before 30th September and grab one month rent free\n",
      "in December.\n",
      "Spend your December budget on gifts and fun instead of rent.\n",
      "\n",
      "With us you’ll find more than just an apartment. You’ll find a place that feels like home from day one, where you can focus on your studies, build lasting\n",
      "friendships, and become part of a vibrant international community. Be part of it.\n",
      "Our apartments are fully furnished with modern, comfor...\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:43,690 [INFO] Tags : [None]\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:52,254 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:52,340 [INFO] Latitude None, longitude : None, Number of Rooms : None\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:52,341 [INFO] Created folder or folder already exists for listing with id; 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:52,342 [INFO] Wrote meta_data file for listing with id; 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:52,343 [INFO] Created metadata file for listing with id ; 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:52,408 [INFO] Trying to scrape a listing from the container\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 17:04:53,557 [INFO] Acquired listing id 1148761\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:53,642 [INFO] Title : None, price : None, area : None, description : None, fulldescription : None\u001b[0m\n",
      "\u001b[32m2025-09-28 17:04:53,652 [INFO] Tags : [None]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 31 doesnt start with http\n",
      "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 17:05:11,123 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 17:05:11,199 [INFO] Latitude None, longitude : None, Number of Rooms : None\u001b[0m\n",
      "\u001b[32m2025-09-28 17:05:11,200 [INFO] Created folder or folder already exists for listing with id; 1148761\u001b[0m\n",
      "\u001b[32m2025-09-28 17:05:11,201 [INFO] Wrote meta_data file for listing with id; 1148761\u001b[0m\n",
      "\u001b[32m2025-09-28 17:05:11,202 [INFO] Created metadata file for listing with id ; 1148761\u001b[0m\n",
      "\u001b[32m2025-09-28 17:05:11,262 [INFO] Trying to scrape a listing from the container\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 17:05:14,843 [INFO] Acquired listing id 1145984\u001b[0m\n",
      "\u001b[32m2025-09-28 17:05:14,935 [INFO] Title : Neonwood Adlershof, price : From\n",
      "€765 /month, area : 17 m²+, description : Cleaning in common areas, fulldescription : Life at Berlin Adlershof\n",
      "295 apartments, 295+ students... be one of us & meet extraordinary people. Adlershof is not only a home to renowned non-university research institutions, six institutes of the Humboldt University and around 1,200 companies that are perfect for internships, but also – and this is the best part – our brand new building. Be a part of our community and meet exciting people like you. Our student residence is just a stone’s throw away from the Humboldt University of Berlin – C...\u001b[0m\n",
      "\u001b[32m2025-09-28 17:05:14,943 [INFO] Tags : [None]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 28 doesnt start with http\n",
      "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\n",
      "containers; 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mscrape_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mscrape_pages\u001b[39m\u001b[34m(driver)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     48\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTrying to Scrape the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpages_scraped\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mth Page\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[43mscrape_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSuccessfuly scraped the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpages_scraped\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mth Page\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     52\u001b[39m     pages_scraped += \u001b[32m1\u001b[39m        \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mscrape_page\u001b[39m\u001b[34m(driver)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscrape_page\u001b[39m(driver):\n\u001b[32m     26\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    For each page, scrape its upper half first.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    Then scroll down a bit, to get more container listings, and scrape the other half\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[43mscrape_half_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     driver.execute_script(\u001b[33m\"\u001b[39m\u001b[33mwindow.scrollBy(0, 2000);\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m     scrape_half_page(driver)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mscrape_half_page\u001b[39m\u001b[34m(driver)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     15\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[33mTrying to scrape a listing from the container\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     listing_data = \u001b[43mscrape_listing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlisting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_window\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ElementClickInterceptedException:\n\u001b[32m     18\u001b[39m     logger.error(\u001b[33m'\u001b[39m\u001b[33mFailed to scrape a listing from the container\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 172\u001b[39m, in \u001b[36mscrape_listing\u001b[39m\u001b[34m(listing, driver, original_window)\u001b[39m\n\u001b[32m    169\u001b[39m tags = get_listing_tags(driver)\n\u001b[32m    170\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTags : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtags\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[43mscrape_listing_photos_and_create_their_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlisting_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m logger.info(\u001b[33m'\u001b[39m\u001b[33mScraped photos\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    175\u001b[39m latitude, longitude, number_of_rooms = get_geo_data(driver)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mscrape_listing_photos_and_create_their_file\u001b[39m\u001b[34m(driver, listing_id)\u001b[39m\n\u001b[32m     71\u001b[39m image_url = image.get_attribute(\u001b[33m'\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m image.get_attribute(\u001b[33m'\u001b[39m\u001b[33mdata-src\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m image.get_attribute(\u001b[33m'\u001b[39m\u001b[33mdata-lazy\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m image.get_attribute(\u001b[33m'\u001b[39m\u001b[33mdata-original\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image_url.startswith(\u001b[33m\"\u001b[39m\u001b[33mhttp\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     image_to_be_written = response.content\n\u001b[32m     78\u001b[39m     image_bytes = \u001b[38;5;28mlen\u001b[39m(image_to_be_written)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\urllib3\\connection.py:704\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    703\u001b[39m     sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m704\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m     server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m    706\u001b[39m     tls_in_tls = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m:return: New socket connection.\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[32m     75\u001b[39m err = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "scrape_pages(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed697776",
   "metadata": {},
   "source": [
    "Next feature testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def go_to_next_page(driver):\n",
    "    logger.info('Trying to go to the next Page')\n",
    "    next_button = driver.find_element(By.CSS_SELECTOR, \"button[aria-label='Go to next page']\")\n",
    "    next_button.click()\n",
    "    logger.info('Clicked on next Page')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a7b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "driver = get_driver()\n",
    "driver.get('https://housinganywhere.com')\n",
    "search_for_place(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d977b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.execute_script(\"window.scrollBy(0, 4000);\")\n",
    "# time.sleep(2)\n",
    "# go_to_next_page(driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c134222",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(200, document.body.scrollHeight);\")\n",
    "\n",
    "#go_to_next_page(driver)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
