{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5695d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Germany'\n",
    "city = 'Berlin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b795d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import selenium\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from housing_functions import get_driver\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pprint\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "import demjson3\n",
    "import os\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "from housing_functions import random_click, get_driver, search_for_place\n",
    "\n",
    "import json\n",
    "\n",
    "import logging\n",
    "import colorlog\n",
    "\n",
    "import sys \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1448b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "255dbd24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98acdb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_listings(driver):\n",
    "    pass\n",
    "    \n",
    "def get_logger():\n",
    "    handler = colorlog.StreamHandler()\n",
    "    formatter = colorlog.ColoredFormatter(\n",
    "        \"%(log_color)s%(asctime)s [%(levelname)s] %(message)s\",\n",
    "        log_colors={\n",
    "            \"DEBUG\": \"cyan\",\n",
    "            \"INFO\": \"green\",\n",
    "            \"WARNING\": \"yellow\",\n",
    "            \"ERROR\": \"red\",\n",
    "            \"CRITICAL\": \"bold_red\",\n",
    "        }\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    \n",
    "    logger = colorlog.getLogger(__name__)\n",
    "    logger.addHandler(logging.FileHandler(\"logging.txt\", encoding=\"utf-8\"))\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "    \n",
    "def accept_cookies(driver):\n",
    "    cookies_button = driver.find_element(By.ID,'onetrust-accept-btn-handler')\n",
    "    cookies_button.click()\n",
    "    \n",
    "    \n",
    "def get_container_listings(container):\n",
    "    container.find_elements(By.CLASS_NAME,'css-1efwqj7-cardLink')\n",
    "    return container\n",
    "\n",
    "\n",
    "def create_directory_for_photos():\n",
    "    if not os.path.exists(f'{country}_{city}_house_photos'):\n",
    "        os.mkdir(f'{country}_{city}_house_photos')\n",
    "\n",
    "\n",
    "\n",
    "def get_geo_data(driver):\n",
    "    try:\n",
    "        latitude, longitude, number_of_rooms = None, None, None\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source,'html.parser')\n",
    "        script_elements = soup.find_all('script', type='application/ld+json')\n",
    "        for script in script_elements:\n",
    "            if script.string:\n",
    "                data = json.loads(script.string)\n",
    "                \n",
    "                if data.get('@type') != 'Accommodation':\n",
    "                    continue\n",
    "                geo_data = data.get('geo')\n",
    "                latitude = geo_data.get('latitude')\n",
    "                longitude = geo_data.get('longitude')\n",
    "                number_of_rooms = data.get('numberOfRooms')         \n",
    "    except :\n",
    "        print('Inside the geo_data function, an Exception has occured')    \n",
    "        return None, None, None\n",
    "    \n",
    "    return latitude, longitude, number_of_rooms\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd01c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_container(driver):\n",
    "    page_container = driver.find_element(By.CLASS_NAME, 'css-wp5dsn-container')\n",
    "    return page_container\n",
    "\n",
    "def get_container_listings(container):\n",
    "     rows = container.find_elements(By.CLASS_NAME,'css-1efwqj7-cardLink')\n",
    "     return rows\n",
    "\n",
    "\n",
    "def scroll_page(driver):\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "\n",
    "def scroll_down_and_wait_2_secs(driver):\n",
    "    driver.execute_script(\"window.scrollBy(0, 4000);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "# def scrape_half_page(driver):\n",
    "    \n",
    "#     listings_data_of_this_page = []\n",
    "    \n",
    "#     container = get_container(driver)\n",
    "#     container_listings = get_container_listings(container)\n",
    "    \n",
    "    \n",
    "#     for listing in container_listings:\n",
    "        \n",
    "#         listing_data =scrape_listing(listing,driver)\n",
    "#         listings_data_of_this_page.append(listing_data)\n",
    "    \n",
    "    \n",
    "#     return listings_data_of_this_page\n",
    "    \n",
    "    \n",
    "def get_listing_tags(driver):\n",
    "    try:\n",
    "        tags_container = driver.find_element(By.CLASS_NAME, 'css-q6fy6c-highlightContainer')\n",
    "        if tags_container:\n",
    "            container_elements = tags_container.find_elements(By.CLASS_NAME, 'css-1e5azn1-highlightItem')\n",
    "            tags = []\n",
    "            for tag_element in container_elements:\n",
    "                tag = tag_element.text\n",
    "                tags.append(tag)\n",
    "            return tags\n",
    "    except:\n",
    "        return [None]\n",
    "        \n",
    "        \n",
    "def scrape_listing_photos_and_create_their_file(driver, listing_id:str):\n",
    "    folder = os.path.join(f'{country}_{city}_house_photos', listing_id)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    images_buttons = driver.find_elements(By.CLASS_NAME,'css-13emeri-tile-tileButton')\n",
    "        \n",
    "    # click on more photos so that more photos are loaded\n",
    "    more_photos_button = driver.find_element(By.CLASS_NAME,'css-13emeri-tile-tileButton')\n",
    "    more_photos_button.click()\n",
    "    images = driver.find_elements(By.TAG_NAME, 'img')   \n",
    "    \n",
    "    unique_images = set(images)\n",
    "\n",
    "    \n",
    "    for i, image in enumerate(unique_images):\n",
    "        size = image.size\n",
    "        height = size['height']\n",
    "        if height >= 45:\n",
    "            image_url = image.get_attribute('src') or image.get_attribute('data-src') or image.get_attribute('data-lazy') or image.get_attribute('data-original')\n",
    "            \n",
    "            if image_url.startswith(\"http\"):\n",
    "\n",
    "                response = requests.get(image_url)\n",
    "                image_to_be_written = response.content\n",
    "                \n",
    "                image_bytes = len(image_to_be_written)\n",
    "                if image_bytes > 100 * 1000:\n",
    "                    with open(os.path.join(folder, f'{listing_id}_{i}.jpg'), 'wb') as f1:\n",
    "                        f1.write(image_to_be_written)\n",
    "            else:\n",
    "                print(f'image {i} doesnt start with http')\n",
    "                print(image_url)\n",
    "\n",
    "    \n",
    "    \n",
    "def create_metadata_file_in_the_listings_folder(listing_id,title,price,area,description,full_description,tags,latitude,longitude,number_of_rooms):\n",
    "    try:\n",
    "        print(listing_id.__class__)\n",
    "        if not isinstance(listing_id,str):\n",
    "            logger.error(f'Listing with id; {listing_id} is not of type; str')\n",
    "            raise ValueError\n",
    "        folder = os.path.join(f'{country}_{city}_house_photos', listing_id)    \n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        logger.info(f'Created folder or folder already exists for listing with id; {listing_id}')\n",
    "        file = os.path.join(folder,'.metadata')\n",
    "        with open (file, 'w', encoding='utf-8') as f1:\n",
    "            meta_data = {\n",
    "                'listing_id' : listing_id,\n",
    "                'title' : title,\n",
    "                'price' : price,\n",
    "                'area' : area,\n",
    "                'description' : description,\n",
    "                'full_description' : full_description,\n",
    "                'latitude' : latitude,\n",
    "                'longitude'  : longitude,\n",
    "                'number_of_rooms' : number_of_rooms\n",
    "            }\n",
    "            for i, tag in enumerate(tags):\n",
    "                key = f'tag_{i}'\n",
    "                meta_data[key] = tag\n",
    "                \n",
    "            json_meta_data = json.dumps(meta_data, ensure_ascii= False, indent=4)\n",
    "            f1.write(json_meta_data)\n",
    "            logger.info(f'Wrote meta_data file for listing with id; {listing_id}')\n",
    "    except Exception :\n",
    "        logger.error(f'Encountered error during the writing of the Meta Data file for listing with id; {listing_id}')\n",
    "        driver.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def get_listing_text_attributes(driver):\n",
    "    try:\n",
    "        title = driver.find_element(By.CLASS_NAME,'css-1ql5bbl').text\n",
    "        price = driver.find_element(By.CLASS_NAME, 'css-1bop1zx-pricingContent').text\n",
    "        area = driver.find_element(By.CLASS_NAME, 'css-2ccjfp').text\n",
    "        description = driver.find_element(By.CLASS_NAME, 'css-31lj5q').text\n",
    "        full_description = driver.find_element(By.CLASS_NAME, 'css-1liw7jd-preWrap-breakWord').text\n",
    "\n",
    "        return title, price, area, description, full_description\n",
    "    \n",
    "    except Exception:\n",
    "        return (None,None,None,None,None,)\n",
    "        \n",
    "\n",
    "\n",
    "def get_listing_id(driver):\n",
    "    current_url = driver.current_url\n",
    "    if '/ut' in current_url:\n",
    "        listing_id = current_url.split('/ut')[1].split('/de')[0]\n",
    "    else:    \n",
    "        listing_id = current_url.split('-')[-1]\n",
    "    return listing_id\n",
    "\n",
    "\n",
    "\n",
    "def scrape_listing(listing, driver, original_window):\n",
    "    try:\n",
    "        listing.click()\n",
    "    except Exception:\n",
    "        logger.error('Didn t manage to sclick on listing.')\n",
    "    \n",
    "    try:\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        listing_id = get_listing_id(driver)\n",
    "        logger.info(f'Acquired listing id {listing_id}')\n",
    "        \n",
    "        title, price, area, description, full_description = get_listing_text_attributes(driver)\n",
    "        logger.info(f'Title : {title}, price : {price}, area : {area}, description : {description}, fulldescription : {full_description}')\n",
    "        \n",
    "        tags = get_listing_tags(driver)\n",
    "        logger.info(f'Tags : {tags}' )\n",
    "        \n",
    "        scrape_listing_photos_and_create_their_file(driver,listing_id)\n",
    "        logger.info('Scraped photos')\n",
    "        \n",
    "        latitude, longitude, number_of_rooms = get_geo_data(driver)\n",
    "        logger.info(f'Latitude {latitude}, longitude : {longitude}, Number of Rooms : {number_of_rooms}')\n",
    "\n",
    "        create_metadata_file_in_the_listings_folder(listing_id,title,price,area,description,full_description,tags,latitude,longitude,number_of_rooms)\n",
    "        logger.info(f'Created metadata file for listing with id ; {listing_id}')\n",
    "    \n",
    "    except Exception:\n",
    "        logger.error('something failed when scraping the data')\n",
    "    finally:\n",
    "        \n",
    "        if driver.current_window_handle != original_window:\n",
    "            driver.close()\n",
    "            driver.switch_to.window(original_window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85857577",
   "metadata": {},
   "source": [
    "<button class=\"MuiButtonBase-root MuiButton-root MuiButton-contained MuiButton-containedPrimary MuiButton-sizeMedium MuiButton-containedSizeMedium MuiButton-fullWidth MuiButton-root MuiButton-contained MuiButton-containedPrimary MuiButton-sizeMedium MuiButton-containedSizeMedium MuiButton-fullWidth css-tkyhxc-button-button\" tabindex=\"0\" type=\"submit\" data-test-locator=\"Search and book\">Search<span class=\"MuiTouchRipple-root css-w0pj6f\"></span></button>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562f5c7",
   "metadata": {},
   "source": [
    "******** PROGRAM STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3a2c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "driver = get_driver()\n",
    "create_directory_for_photos()\n",
    "#ipython = get_ipython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fbb9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tries = 0\n",
    "while True:\n",
    "    try:\n",
    "        tries += 1\n",
    "        if tries == 2:\n",
    "            os.execv(sys.executable, ['python'] + sys.argv)\n",
    "\n",
    "        search_for_place(driver)\n",
    "        accept_cookies(driver)\n",
    "        break\n",
    "\n",
    "    except Exception :\n",
    "        logger.error('Exception while trying to search for place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ba4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_half_page(driver):\n",
    "    try:\n",
    "        logger.info('Trying to get the container element')\n",
    "        container = driver.find_element(By.CLASS_NAME, 'css-wp5dsn-container')\n",
    "        \n",
    "        scroll_page(driver)\n",
    "        logger.info('Trying to get the container\\'s listings')\n",
    "        container_listings = get_container_listings(container)\n",
    "        \n",
    "        original_window = driver.current_window_handle\n",
    "\n",
    "        logger.info('Iterating over the container\\'s listings')\n",
    "        for  listing in container_listings:\n",
    "            try:\n",
    "                logger.info('Trying to scrape a listing from the container')\n",
    "                listing_data = scrape_listing(listing, driver, original_window)\n",
    "            except ElementClickInterceptedException:\n",
    "                logger.error('Failed to scrape a listing from the container')\n",
    "    except Exception :\n",
    "        logger.error('Undefined error')\n",
    "\n",
    "    finally:\n",
    "        print('containers;', len(container_listings))\n",
    "\n",
    "def scrape_page(driver):\n",
    "    '''\n",
    "    For each page, scrape its upper half first.\n",
    "    Then scroll down a bit, to get more container listings, and scrape the other half\n",
    "    '''\n",
    "    scrape_half_page(driver)\n",
    "    driver.execute_script(\"window.scrollBy(0, 2000);\")\n",
    "    scrape_half_page(driver)\n",
    "\n",
    "\n",
    "def go_to_next_page(driver):\n",
    "    logger.info('Trying to go to the next Page')\n",
    "    next_button = driver.find_element(By.CSS_SELECTOR, \"button[aria-label='Go to next page']\")\n",
    "    next_button.click()\n",
    "    logger.info('Clicked on next Page')\n",
    "\n",
    "\n",
    "def scrape_pages(driver):\n",
    "    logger.info('Started Scraping Pages')\n",
    "    pages_scraped = 0\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                logger.info(f'Trying to Scrape the {pages_scraped}th Page')\n",
    "                #scrape_page(driver)\n",
    "                logger.info(f'Successfuly scraped the {pages_scraped}th Page')\n",
    "                \n",
    "                pages_scraped += 1        \n",
    "                \n",
    "                logger.info(f'Trying to click on the {pages_scraped}th Page')\n",
    "                \n",
    "                scroll_down_and_wait_2_secs(driver)\n",
    "\n",
    "                go_to_next_page(driver)\n",
    "                \n",
    "                if pages_scraped == 100: # we do not need to scrape more than 100 pages\n",
    "                    logger.info('Scraped 100 pages, Now exiting')\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                logger.error(f'Encounterd Error during the scraping of the {pages_scraped}th Page', exc_info=True)\n",
    "    finally:\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c14aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:42:11,411 [INFO] Started Scraping Pages\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:11,412 [INFO] Trying to Scrape the 0th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:11,413 [INFO] Successfuly scraped the 0th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:11,414 [INFO] Trying to click on the 1th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:14,438 [INFO] Trying to go to the next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:14,490 [INFO] Clicked on next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:14,490 [INFO] Trying to Scrape the 1th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:14,491 [INFO] Successfuly scraped the 1th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:14,492 [INFO] Trying to click on the 2th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:16,666 [INFO] Trying to go to the next Page\u001b[0m\n",
      "\u001b[31m2025-09-28 16:42:17,726 [ERROR] Encounterd Error during the scraping of the 2th Page\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_24668\\3468109297.py\", line 58, in scrape_pages\n",
      "    go_to_next_page(driver)\n",
      "  File \"C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_24668\\3468109297.py\", line 38, in go_to_next_page\n",
      "    next_button.click()\n",
      "  File \"c:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\", line 119, in click\n",
      "    self._execute(Command.CLICK_ELEMENT)\n",
      "  File \"c:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\", line 572, in _execute\n",
      "    return self._parent.execute(command, params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 429, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"c:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\", line 232, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.ElementClickInterceptedException: Message: element click intercepted: Element is not clickable at point (781, 4357)\n",
      "  (Session info: chrome=140.0.7339.186)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff6a3941eb5+80197]\n",
      "\tGetHandleVerifier [0x0x7ff6a3941f10+80288]\n",
      "\t(No symbol) [0x0x7ff6a36c02fa]\n",
      "\t(No symbol) [0x0x7ff6a371fe69]\n",
      "\t(No symbol) [0x0x7ff6a371d7ee]\n",
      "\t(No symbol) [0x0x7ff6a371a731]\n",
      "\t(No symbol) [0x0x7ff6a3719620]\n",
      "\t(No symbol) [0x0x7ff6a370abc8]\n",
      "\t(No symbol) [0x0x7ff6a374037a]\n",
      "\t(No symbol) [0x0x7ff6a370a456]\n",
      "\t(No symbol) [0x0x7ff6a3740590]\n",
      "\t(No symbol) [0x0x7ff6a37687fb]\n",
      "\t(No symbol) [0x0x7ff6a3740153]\n",
      "\t(No symbol) [0x0x7ff6a3708b02]\n",
      "\t(No symbol) [0x0x7ff6a37098d3]\n",
      "\tGetHandleVerifier [0x0x7ff6a3bfe83d+2949837]\n",
      "\tGetHandleVerifier [0x0x7ff6a3bf8c6a+2926330]\n",
      "\tGetHandleVerifier [0x0x7ff6a3c186c7+3055959]\n",
      "\tGetHandleVerifier [0x0x7ff6a395cfee+191102]\n",
      "\tGetHandleVerifier [0x0x7ff6a39650af+224063]\n",
      "\tGetHandleVerifier [0x0x7ff6a394af64+117236]\n",
      "\tGetHandleVerifier [0x0x7ff6a394b119+117673]\n",
      "\tGetHandleVerifier [0x0x7ff6a39310a8+11064]\n",
      "\tBaseThreadInitThunk [0x0x7ffac4f0e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffac5ec8d9c+44]\n",
      "\n",
      "\u001b[32m2025-09-28 16:42:17,729 [INFO] Trying to Scrape the 2th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:17,729 [INFO] Successfuly scraped the 2th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:17,730 [INFO] Trying to click on the 3th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:19,735 [INFO] Trying to go to the next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:19,789 [INFO] Clicked on next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:19,791 [INFO] Trying to Scrape the 3th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:19,791 [INFO] Successfuly scraped the 3th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:19,792 [INFO] Trying to click on the 4th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:21,956 [INFO] Trying to go to the next Page\u001b[0m\n",
      "\u001b[31m2025-09-28 16:42:23,047 [ERROR] Encounterd Error during the scraping of the 4th Page\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_24668\\3468109297.py\", line 58, in scrape_pages\n",
      "    go_to_next_page(driver)\n",
      "  File \"C:\\Users\\steve\\AppData\\Local\\Temp\\ipykernel_24668\\3468109297.py\", line 38, in go_to_next_page\n",
      "    next_button.click()\n",
      "  File \"c:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\", line 119, in click\n",
      "    self._execute(Command.CLICK_ELEMENT)\n",
      "  File \"c:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\", line 572, in _execute\n",
      "    return self._parent.execute(command, params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\", line 429, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"c:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\", line 232, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.ElementClickInterceptedException: Message: element click intercepted: Element is not clickable at point (781, 4326)\n",
      "  (Session info: chrome=140.0.7339.186)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x7ff6a3941eb5+80197]\n",
      "\tGetHandleVerifier [0x0x7ff6a3941f10+80288]\n",
      "\t(No symbol) [0x0x7ff6a36c02fa]\n",
      "\t(No symbol) [0x0x7ff6a371fe69]\n",
      "\t(No symbol) [0x0x7ff6a371d7ee]\n",
      "\t(No symbol) [0x0x7ff6a371a731]\n",
      "\t(No symbol) [0x0x7ff6a3719620]\n",
      "\t(No symbol) [0x0x7ff6a370abc8]\n",
      "\t(No symbol) [0x0x7ff6a374037a]\n",
      "\t(No symbol) [0x0x7ff6a370a456]\n",
      "\t(No symbol) [0x0x7ff6a3740590]\n",
      "\t(No symbol) [0x0x7ff6a37687fb]\n",
      "\t(No symbol) [0x0x7ff6a3740153]\n",
      "\t(No symbol) [0x0x7ff6a3708b02]\n",
      "\t(No symbol) [0x0x7ff6a37098d3]\n",
      "\tGetHandleVerifier [0x0x7ff6a3bfe83d+2949837]\n",
      "\tGetHandleVerifier [0x0x7ff6a3bf8c6a+2926330]\n",
      "\tGetHandleVerifier [0x0x7ff6a3c186c7+3055959]\n",
      "\tGetHandleVerifier [0x0x7ff6a395cfee+191102]\n",
      "\tGetHandleVerifier [0x0x7ff6a39650af+224063]\n",
      "\tGetHandleVerifier [0x0x7ff6a394af64+117236]\n",
      "\tGetHandleVerifier [0x0x7ff6a394b119+117673]\n",
      "\tGetHandleVerifier [0x0x7ff6a39310a8+11064]\n",
      "\tBaseThreadInitThunk [0x0x7ffac4f0e8d7+23]\n",
      "\tRtlUserThreadStart [0x0x7ffac5ec8d9c+44]\n",
      "\n",
      "\u001b[32m2025-09-28 16:42:23,048 [INFO] Trying to Scrape the 4th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:23,049 [INFO] Successfuly scraped the 4th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:42:23,049 [INFO] Trying to click on the 5th Page\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "scrape_pages(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed697776",
   "metadata": {},
   "source": [
    "Next feature testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def go_to_next_page(driver):\n",
    "    logger.info('Trying to go to the next Page')\n",
    "    next_button = driver.find_element(By.CSS_SELECTOR, \"button[aria-label='Go to next page']\")\n",
    "    next_button.click()\n",
    "    logger.info('Clicked on next Page')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a7b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "driver = get_driver()\n",
    "driver.get('https://housinganywhere.com')\n",
    "search_for_place(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d977b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:39:07,042 [INFO] Trying to go to the next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,042 [INFO] Trying to go to the next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,042 [INFO] Trying to go to the next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,042 [INFO] Trying to go to the next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,093 [INFO] Clicked on next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,093 [INFO] Clicked on next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,093 [INFO] Clicked on next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,093 [INFO] Clicked on next Page\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# driver.execute_script(\"window.scrollBy(0, 4000);\")\n",
    "# time.sleep(2)\n",
    "# go_to_next_page(driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c134222",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(200, document.body.scrollHeight);\")\n",
    "\n",
    "#go_to_next_page(driver)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
