{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5695d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Germany'\n",
    "city = 'Berlin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b795d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import selenium\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from housing_functions import get_driver\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pprint\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "import demjson3\n",
    "import os\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "from housing_functions import random_click, get_driver, search_for_place\n",
    "\n",
    "import json\n",
    "\n",
    "import logging\n",
    "import colorlog\n",
    "\n",
    "import sys \n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1448b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "255dbd24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98acdb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_listings(driver):\n",
    "    pass\n",
    "    \n",
    "def get_logger():\n",
    "    handler = colorlog.StreamHandler()\n",
    "    formatter = colorlog.ColoredFormatter(\n",
    "        \"%(log_color)s%(asctime)s [%(levelname)s] %(message)s\",\n",
    "        log_colors={\n",
    "            \"DEBUG\": \"cyan\",\n",
    "            \"INFO\": \"green\",\n",
    "            \"WARNING\": \"yellow\",\n",
    "            \"ERROR\": \"red\",\n",
    "            \"CRITICAL\": \"bold_red\",\n",
    "        }\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    \n",
    "    logger = colorlog.getLogger(__name__)\n",
    "    logger.addHandler(logging.FileHandler(\"logging.txt\", encoding=\"utf-8\"))\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    return logger\n",
    "    \n",
    "def accept_cookies(driver):\n",
    "    cookies_button = driver.find_element(By.ID,'onetrust-accept-btn-handler')\n",
    "    cookies_button.click()\n",
    "    \n",
    "    \n",
    "def get_container_listings(container):\n",
    "    container.find_elements(By.CLASS_NAME,'css-1efwqj7-cardLink')\n",
    "    return container\n",
    "\n",
    "\n",
    "def create_directory_for_photos():\n",
    "    if not os.path.exists(f'{country}_{city}_house_photos'):\n",
    "        os.mkdir(f'{country}_{city}_house_photos')\n",
    "\n",
    "\n",
    "\n",
    "def get_geo_data(driver):\n",
    "    try:\n",
    "        latitude, longitude, number_of_rooms = None, None, None\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source,'html.parser')\n",
    "        script_elements = soup.find_all('script', type='application/ld+json')\n",
    "        for script in script_elements:\n",
    "            if script.string:\n",
    "                data = json.loads(script.string)\n",
    "                \n",
    "                if data.get('@type') != 'Accommodation':\n",
    "                    continue\n",
    "                geo_data = data.get('geo')\n",
    "                latitude = geo_data.get('latitude')\n",
    "                longitude = geo_data.get('longitude')\n",
    "                number_of_rooms = data.get('numberOfRooms')         \n",
    "    except :\n",
    "        print('Inside the geo_data function, an Exception has occured')    \n",
    "        return None, None, None\n",
    "    \n",
    "    return latitude, longitude, number_of_rooms\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcd01c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_container(driver):\n",
    "    page_container = driver.find_element(By.CLASS_NAME, 'css-wp5dsn-container')\n",
    "    return page_container\n",
    "\n",
    "def get_container_listings(container):\n",
    "     rows = container.find_elements(By.CLASS_NAME,'css-1efwqj7-cardLink')\n",
    "     return rows\n",
    "\n",
    "\n",
    "def scroll_page(driver):\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "\n",
    "def scroll_down_and_wait_2_secs(driver):\n",
    "    driver.execute_script(\"window.scrollBy(0, 4000);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "# def scrape_half_page(driver):\n",
    "    \n",
    "#     listings_data_of_this_page = []\n",
    "    \n",
    "#     container = get_container(driver)\n",
    "#     container_listings = get_container_listings(container)\n",
    "    \n",
    "    \n",
    "#     for listing in container_listings:\n",
    "        \n",
    "#         listing_data =scrape_listing(listing,driver)\n",
    "#         listings_data_of_this_page.append(listing_data)\n",
    "    \n",
    "    \n",
    "#     return listings_data_of_this_page\n",
    "    \n",
    "    \n",
    "def get_listing_tags(driver):\n",
    "    try:\n",
    "        tags_container = driver.find_element(By.CLASS_NAME, 'css-q6fy6c-highlightContainer')\n",
    "        if tags_container:\n",
    "            container_elements = tags_container.find_elements(By.CLASS_NAME, 'css-1e5azn1-highlightItem')\n",
    "            tags = []\n",
    "            for tag_element in container_elements:\n",
    "                tag = tag_element.text\n",
    "                tags.append(tag)\n",
    "            return tags\n",
    "    except:\n",
    "        return [None]\n",
    "        \n",
    "        \n",
    "def scrape_listing_photos_and_create_their_file(driver, listing_id:str):\n",
    "    folder = os.path.join(f'{country}_{city}_house_photos', listing_id)\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    #images_buttons = driver.find_elements(By.CLASS_NAME,'css-13emeri-tile-tileButton')\n",
    "        \n",
    "    # click on more photos so that more photos are loaded\n",
    "    more_photos_button = driver.find_element(By.CLASS_NAME,'css-13emeri-tile-tileButton')\n",
    "    more_photos_button.click()\n",
    "    images = driver.find_elements(By.TAG_NAME, 'img')   \n",
    "    \n",
    "    unique_images = set(images)\n",
    "\n",
    "    \n",
    "    for i, image in enumerate(unique_images):\n",
    "        size = image.size\n",
    "        height = size['height']\n",
    "        if height >= 45:\n",
    "            image_url = image.get_attribute('src') or image.get_attribute('data-src') or image.get_attribute('data-lazy') or image.get_attribute('data-original')\n",
    "            \n",
    "            if image_url.startswith(\"http\"):\n",
    "\n",
    "                response = requests.get(image_url)\n",
    "                image_to_be_written = response.content\n",
    "                \n",
    "                image_bytes = len(image_to_be_written)\n",
    "                if image_bytes > 100 * 1000:\n",
    "                    with open(os.path.join(folder, f'{listing_id}_{i}.jpg'), 'wb') as f1:\n",
    "                        f1.write(image_to_be_written)\n",
    "            else:\n",
    "                print(f'image {i} doesnt start with http')\n",
    "                print(image_url)\n",
    "\n",
    "    \n",
    "    \n",
    "def create_metadata_file_in_the_listings_folder(listing_id,title,price,area,description,full_description,tags,latitude,longitude,number_of_rooms):\n",
    "    try:\n",
    "        print(listing_id.__class__)\n",
    "        if not isinstance(listing_id,str):\n",
    "            logger.error(f'Listing with id; {listing_id} is not of type; str')\n",
    "            raise ValueError\n",
    "        folder = os.path.join(f'{country}_{city}_house_photos', listing_id)    \n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        logger.info(f'Created folder or folder already exists for listing with id; {listing_id}')\n",
    "        file = os.path.join(folder,'.metadata')\n",
    "        with open (file, 'w', encoding='utf-8') as f1:\n",
    "            meta_data = {\n",
    "                'listing_id' : listing_id,\n",
    "                'title' : title,\n",
    "                'price' : price,\n",
    "                'area' : area,\n",
    "                'description' : description,\n",
    "                'full_description' : full_description,\n",
    "                'latitude' : latitude,\n",
    "                'longitude'  : longitude,\n",
    "                'number_of_rooms' : number_of_rooms\n",
    "            }\n",
    "            for i, tag in enumerate(tags):\n",
    "                key = f'tag_{i}'\n",
    "                meta_data[key] = tag\n",
    "                \n",
    "            json_meta_data = json.dumps(meta_data, ensure_ascii= False, indent=4)\n",
    "            f1.write(json_meta_data)\n",
    "            logger.info(f'Wrote meta_data file for listing with id; {listing_id}')\n",
    "    except Exception :\n",
    "        logger.error(f'Encountered error during the writing of the Meta Data file for listing with id; {listing_id}')\n",
    "        driver.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def get_listing_text_attributes(driver):\n",
    "    try:\n",
    "        title = driver.find_element(By.CLASS_NAME,'css-1ql5bbl').text\n",
    "        price = driver.find_element(By.CLASS_NAME, 'css-1bop1zx-pricingContent').text\n",
    "        area = driver.find_element(By.CLASS_NAME, 'css-2ccjfp').text\n",
    "        description = driver.find_element(By.CLASS_NAME, 'css-31lj5q').text\n",
    "        full_description = driver.find_element(By.CLASS_NAME, 'css-1liw7jd-preWrap-breakWord').text\n",
    "\n",
    "        return title, price, area, description, full_description\n",
    "    \n",
    "    except Exception:\n",
    "        return (None,None,None,None,None,)\n",
    "        \n",
    "\n",
    "\n",
    "def get_listing_id(driver):\n",
    "    current_url = driver.current_url\n",
    "    if '/ut' in current_url:\n",
    "        listing_id = current_url.split('/ut')[1].split('/de')[0]\n",
    "    else:    \n",
    "        listing_id = current_url.split('-')[-1]\n",
    "    return listing_id\n",
    "\n",
    "\n",
    "\n",
    "def scrape_listing(listing, driver, original_window):\n",
    "    try:\n",
    "        listing.click()\n",
    "    except Exception:\n",
    "        logger.error('Didn t manage to sclick on listing.')\n",
    "    \n",
    "    try:\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "        listing_id = get_listing_id(driver)\n",
    "        logger.info(f'Acquired listing id {listing_id}')\n",
    "        \n",
    "        title, price, area, description, full_description = get_listing_text_attributes(driver)\n",
    "        logger.info(f'Title : {title}, price : {price}, area : {area}, description : {description}, fulldescription : {full_description}')\n",
    "        \n",
    "        tags = get_listing_tags(driver)\n",
    "        logger.info(f'Tags : {tags}' )\n",
    "        \n",
    "        scrape_listing_photos_and_create_their_file(driver,listing_id)\n",
    "        logger.info('Scraped photos')\n",
    "        \n",
    "        latitude, longitude, number_of_rooms = get_geo_data(driver)\n",
    "        logger.info(f'Latitude {latitude}, longitude : {longitude}, Number of Rooms : {number_of_rooms}')\n",
    "\n",
    "        create_metadata_file_in_the_listings_folder(listing_id,title,price,area,description,full_description,tags,latitude,longitude,number_of_rooms)\n",
    "        logger.info(f'Created metadata file for listing with id ; {listing_id}')\n",
    "    \n",
    "    except Exception:\n",
    "        logger.error('something failed when scraping the data')\n",
    "    finally:\n",
    "        \n",
    "        if driver.current_window_handle != original_window:\n",
    "            driver.close()\n",
    "            driver.switch_to.window(original_window)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85857577",
   "metadata": {},
   "source": [
    "<button class=\"MuiButtonBase-root MuiButton-root MuiButton-contained MuiButton-containedPrimary MuiButton-sizeMedium MuiButton-containedSizeMedium MuiButton-fullWidth MuiButton-root MuiButton-contained MuiButton-containedPrimary MuiButton-sizeMedium MuiButton-containedSizeMedium MuiButton-fullWidth css-tkyhxc-button-button\" tabindex=\"0\" type=\"submit\" data-test-locator=\"Search and book\">Search<span class=\"MuiTouchRipple-root css-w0pj6f\"></span></button>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562f5c7",
   "metadata": {},
   "source": [
    "******** PROGRAM STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3a2c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "driver = get_driver()\n",
    "create_directory_for_photos()\n",
    "#ipython = get_ipython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fbb9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tries = 0\n",
    "while True:\n",
    "    try:\n",
    "        tries += 1\n",
    "        if tries == 2:\n",
    "            os.execv(sys.executable, ['python'] + sys.argv)\n",
    "\n",
    "        search_for_place(driver)\n",
    "        accept_cookies(driver)\n",
    "        break\n",
    "\n",
    "    except Exception :\n",
    "        logger.error('Exception while trying to search for place')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56ba4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_half_page(driver):\n",
    "    try:\n",
    "        logger.info('Trying to get the container element')\n",
    "        container = driver.find_element(By.CLASS_NAME, 'css-wp5dsn-container')\n",
    "        \n",
    "        scroll_page(driver)\n",
    "        logger.info('Trying to get the container\\'s listings')\n",
    "        container_listings = get_container_listings(container)\n",
    "        \n",
    "        original_window = driver.current_window_handle\n",
    "\n",
    "        logger.info('Iterating over the container\\'s listings')\n",
    "        for  listing in container_listings:\n",
    "            try:\n",
    "                logger.info('Trying to scrape a listing from the container')\n",
    "                listing_data = scrape_listing(listing, driver, original_window)\n",
    "            except ElementClickInterceptedException:\n",
    "                logger.error('Failed to scrape a listing from the container')\n",
    "    except Exception :\n",
    "        logger.error('Undefined error')\n",
    "\n",
    "    finally:\n",
    "        print('containers;', len(container_listings))\n",
    "\n",
    "def scrape_page(driver):\n",
    "    '''\n",
    "    For each page, scrape its upper half first.\n",
    "    Then scroll down a bit, to get more container listings, and scrape the other half\n",
    "    '''\n",
    "    scrape_half_page(driver)\n",
    "    driver.execute_script(\"window.scrollBy(0, 2000);\")\n",
    "    scrape_half_page(driver)\n",
    "\n",
    "\n",
    "def go_to_next_page(driver):\n",
    "    logger.info('Trying to go to the next Page')\n",
    "    next_button = driver.find_element(By.CSS_SELECTOR, \"button[aria-label='Go to next page']\")\n",
    "    next_button.click()\n",
    "    logger.info('Clicked on next Page')\n",
    "\n",
    "\n",
    "def scrape_pages(driver):\n",
    "    logger.info('Started Scraping Pages')\n",
    "    pages_scraped = 0\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                logger.info(f'Trying to Scrape the {pages_scraped}th Page')\n",
    "                scrape_page(driver)\n",
    "                logger.info(f'Successfuly scraped the {pages_scraped}th Page')\n",
    "                \n",
    "                pages_scraped += 1        \n",
    "                \n",
    "                logger.info(f'Trying to click on the {pages_scraped}th Page')\n",
    "                \n",
    "                scroll_down_and_wait_2_secs(driver)\n",
    "\n",
    "                go_to_next_page(driver)\n",
    "                \n",
    "                if pages_scraped == 100: # we do not need to scrape more than 100 pages\n",
    "                    logger.info('Scraped 100 pages, Now exiting')\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                logger.error(f'Encounterd Error during the scraping of the {pages_scraped}th Page', exc_info=True)\n",
    "    finally:\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13c14aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:49:16,484 [INFO] Started Scraping Pages\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:16,484 [INFO] Started Scraping Pages\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:16,486 [INFO] Trying to Scrape the 0th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:16,486 [INFO] Trying to Scrape the 0th Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:16,487 [INFO] Trying to get the container element\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:16,487 [INFO] Trying to get the container element\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:18,603 [INFO] Trying to get the container's listings\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:18,603 [INFO] Trying to get the container's listings\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:18,613 [INFO] Iterating over the container's listings\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:18,613 [INFO] Iterating over the container's listings\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:18,614 [INFO] Trying to scrape a listing from the container\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:18,614 [INFO] Trying to scrape a listing from the container\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:20,576 [INFO] Acquired listing id 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:20,576 [INFO] Acquired listing id 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:20,669 [INFO] Title : Studyo, price : From\n",
      "€850 /month, area : 16 m²+, description : Cleaning in common areas, fulldescription : BOOK NOW & get the December rent for free!*\n",
      "\n",
      "Find your preferred room, book online before 30th September and grab one month rent free\n",
      "in December.\n",
      "Spend your December budget on gifts and fun instead of rent.\n",
      "\n",
      "With us you’ll find more than just an apartment. You’ll find a place that feels like home from day one, where you can focus on your studies, build lasting\n",
      "friendships, and become part of a vibrant international community. Be part of it.\n",
      "Our apartments are fully furnished with modern, comfor...\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:20,669 [INFO] Title : Studyo, price : From\n",
      "€850 /month, area : 16 m²+, description : Cleaning in common areas, fulldescription : BOOK NOW & get the December rent for free!*\n",
      "\n",
      "Find your preferred room, book online before 30th September and grab one month rent free\n",
      "in December.\n",
      "Spend your December budget on gifts and fun instead of rent.\n",
      "\n",
      "With us you’ll find more than just an apartment. You’ll find a place that feels like home from day one, where you can focus on your studies, build lasting\n",
      "friendships, and become part of a vibrant international community. Be part of it.\n",
      "Our apartments are fully furnished with modern, comfor...\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:20,678 [INFO] Tags : [None]\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:20,678 [INFO] Tags : [None]\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:29,216 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:29,216 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:29,293 [INFO] Latitude None, longitude : None, Number of Rooms : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:29,293 [INFO] Latitude None, longitude : None, Number of Rooms : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:29,295 [INFO] Created folder or folder already exists for listing with id; 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:29,295 [INFO] Created folder or folder already exists for listing with id; 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:29,296 [INFO] Wrote meta_data file for listing with id; 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:29,296 [INFO] Wrote meta_data file for listing with id; 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:29,298 [INFO] Created metadata file for listing with id ; 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:29,298 [INFO] Created metadata file for listing with id ; 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:29,363 [INFO] Trying to scrape a listing from the container\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:29,363 [INFO] Trying to scrape a listing from the container\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:49:30,610 [INFO] Acquired listing id 1148761\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:30,610 [INFO] Acquired listing id 1148761\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:30,689 [INFO] Title : None, price : None, area : None, description : None, fulldescription : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:30,689 [INFO] Title : None, price : None, area : None, description : None, fulldescription : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:30,697 [INFO] Tags : [None]\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:30,697 [INFO] Tags : [None]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 40 doesnt start with http\n",
      "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:49:48,620 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:48,620 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:48,695 [INFO] Latitude None, longitude : None, Number of Rooms : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:48,695 [INFO] Latitude None, longitude : None, Number of Rooms : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:48,696 [INFO] Created folder or folder already exists for listing with id; 1148761\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:48,696 [INFO] Created folder or folder already exists for listing with id; 1148761\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:48,698 [INFO] Wrote meta_data file for listing with id; 1148761\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:48,698 [INFO] Wrote meta_data file for listing with id; 1148761\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:48,699 [INFO] Created metadata file for listing with id ; 1148761\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:48,699 [INFO] Created metadata file for listing with id ; 1148761\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:48,762 [INFO] Trying to scrape a listing from the container\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:48,762 [INFO] Trying to scrape a listing from the container\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:49:50,977 [INFO] Acquired listing id 551278\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:50,977 [INFO] Acquired listing id 551278\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:51,006 [INFO] Title : None, price : None, area : None, description : None, fulldescription : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:51,006 [INFO] Title : None, price : None, area : None, description : None, fulldescription : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:51,054 [INFO] Tags : ['Entire apartment', 'Property: 27 m²', 'Furnished', 'Space for 2 people', '1 bedroom']\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:51,054 [INFO] Tags : ['Entire apartment', 'Property: 27 m²', 'Furnished', 'Space for 2 people', '1 bedroom']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 33 doesnt start with http\n",
      "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:49:59,578 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:59,578 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:59,636 [INFO] Latitude 52.5097, longitude : 13.42231, Number of Rooms : 1\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:59,636 [INFO] Latitude 52.5097, longitude : 13.42231, Number of Rooms : 1\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:59,637 [INFO] Created folder or folder already exists for listing with id; 551278\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:59,637 [INFO] Created folder or folder already exists for listing with id; 551278\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:59,639 [INFO] Wrote meta_data file for listing with id; 551278\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:59,639 [INFO] Wrote meta_data file for listing with id; 551278\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:59,640 [INFO] Created metadata file for listing with id ; 551278\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:59,640 [INFO] Created metadata file for listing with id ; 551278\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:59,707 [INFO] Trying to scrape a listing from the container\u001b[0m\n",
      "\u001b[32m2025-09-28 16:49:59,707 [INFO] Trying to scrape a listing from the container\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:50:02,007 [INFO] Acquired listing id 1030747\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:02,007 [INFO] Acquired listing id 1030747\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:02,038 [INFO] Title : None, price : None, area : None, description : None, fulldescription : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:02,038 [INFO] Title : None, price : None, area : None, description : None, fulldescription : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:02,090 [INFO] Tags : ['Entire apartment', 'Property: 23 m²', 'Furnished', 'Space for 1 person', 'Studio', 'Flexible Cancellation']\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:02,090 [INFO] Tags : ['Entire apartment', 'Property: 23 m²', 'Furnished', 'Space for 1 person', 'Studio', 'Flexible Cancellation']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 20 doesnt start with http\n",
      "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:50:24,221 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:24,221 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:24,284 [INFO] Latitude 52.564, longitude : 13.40742, Number of Rooms : 0\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:24,284 [INFO] Latitude 52.564, longitude : 13.40742, Number of Rooms : 0\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:24,286 [INFO] Created folder or folder already exists for listing with id; 1030747\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:24,286 [INFO] Created folder or folder already exists for listing with id; 1030747\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:24,288 [INFO] Wrote meta_data file for listing with id; 1030747\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:24,288 [INFO] Wrote meta_data file for listing with id; 1030747\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:24,289 [INFO] Created metadata file for listing with id ; 1030747\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:24,289 [INFO] Created metadata file for listing with id ; 1030747\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:24,360 [INFO] Trying to scrape a listing from the container\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:24,360 [INFO] Trying to scrape a listing from the container\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:50:25,870 [INFO] Acquired listing id 890396\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:25,870 [INFO] Acquired listing id 890396\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:25,934 [INFO] Title : None, price : None, area : None, description : None, fulldescription : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:25,934 [INFO] Title : None, price : None, area : None, description : None, fulldescription : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:25,991 [INFO] Tags : ['Entire apartment', 'Property: 20 m²', 'Furnished', 'Space for 1 person', 'Studio', 'Flexible Cancellation']\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:25,991 [INFO] Tags : ['Entire apartment', 'Property: 20 m²', 'Furnished', 'Space for 1 person', 'Studio', 'Flexible Cancellation']\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:38,378 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:38,378 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:38,452 [INFO] Latitude 52.46486, longitude : 13.51172, Number of Rooms : 0\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:38,452 [INFO] Latitude 52.46486, longitude : 13.51172, Number of Rooms : 0\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:38,454 [INFO] Created folder or folder already exists for listing with id; 890396\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:38,454 [INFO] Created folder or folder already exists for listing with id; 890396\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:38,456 [INFO] Wrote meta_data file for listing with id; 890396\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:38,456 [INFO] Wrote meta_data file for listing with id; 890396\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:38,457 [INFO] Created metadata file for listing with id ; 890396\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:38,457 [INFO] Created metadata file for listing with id ; 890396\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:38,514 [INFO] Trying to scrape a listing from the container\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:38,514 [INFO] Trying to scrape a listing from the container\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 37 doesnt start with http\n",
      "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:50:42,605 [INFO] Acquired listing id 1145984\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:42,605 [INFO] Acquired listing id 1145984\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:42,691 [INFO] Title : Neonwood Adlershof, price : From\n",
      "€765 /month, area : 17 m²+, description : Cleaning in common areas, fulldescription : Life at Berlin Adlershof\n",
      "295 apartments, 295+ students... be one of us & meet extraordinary people. Adlershof is not only a home to renowned non-university research institutions, six institutes of the Humboldt University and around 1,200 companies that are perfect for internships, but also – and this is the best part – our brand new building. Be a part of our community and meet exciting people like you. Our student residence is just a stone’s throw away from the Humboldt University of Berlin – C...\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:42,691 [INFO] Title : Neonwood Adlershof, price : From\n",
      "€765 /month, area : 17 m²+, description : Cleaning in common areas, fulldescription : Life at Berlin Adlershof\n",
      "295 apartments, 295+ students... be one of us & meet extraordinary people. Adlershof is not only a home to renowned non-university research institutions, six institutes of the Humboldt University and around 1,200 companies that are perfect for internships, but also – and this is the best part – our brand new building. Be a part of our community and meet exciting people like you. Our student residence is just a stone’s throw away from the Humboldt University of Berlin – C...\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:42,701 [INFO] Tags : [None]\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:42,701 [INFO] Tags : [None]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 26 doesnt start with http\n",
      "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:50:55,875 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:55,875 [INFO] Scraped photos\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:56,029 [INFO] Latitude None, longitude : None, Number of Rooms : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:56,029 [INFO] Latitude None, longitude : None, Number of Rooms : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:56,030 [INFO] Created folder or folder already exists for listing with id; 1145984\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:56,030 [INFO] Created folder or folder already exists for listing with id; 1145984\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:56,032 [INFO] Wrote meta_data file for listing with id; 1145984\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:56,032 [INFO] Wrote meta_data file for listing with id; 1145984\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:56,032 [INFO] Created metadata file for listing with id ; 1145984\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:56,032 [INFO] Created metadata file for listing with id ; 1145984\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:56,103 [INFO] Trying to scrape a listing from the container\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:56,103 [INFO] Trying to scrape a listing from the container\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025-09-28 16:50:57,137 [ERROR] Didn t manage to sclick on listing.\u001b[0m\n",
      "\u001b[31m2025-09-28 16:50:57,137 [ERROR] Didn t manage to sclick on listing.\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:57,143 [INFO] Acquired listing id Germany\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:57,143 [INFO] Acquired listing id Germany\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:57,152 [INFO] Title : None, price : None, area : None, description : None, fulldescription : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:57,152 [INFO] Title : None, price : None, area : None, description : None, fulldescription : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:57,161 [INFO] Tags : [None]\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:57,161 [INFO] Tags : [None]\u001b[0m\n",
      "\u001b[31m2025-09-28 16:50:57,170 [ERROR] something failed when scraping the data\u001b[0m\n",
      "\u001b[31m2025-09-28 16:50:57,170 [ERROR] something failed when scraping the data\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:57,172 [INFO] Trying to scrape a listing from the container\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:57,172 [INFO] Trying to scrape a listing from the container\u001b[0m\n",
      "\u001b[31m2025-09-28 16:50:58,197 [ERROR] Didn t manage to sclick on listing.\u001b[0m\n",
      "\u001b[31m2025-09-28 16:50:58,197 [ERROR] Didn t manage to sclick on listing.\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,204 [INFO] Acquired listing id Germany\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,204 [INFO] Acquired listing id Germany\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,211 [INFO] Title : None, price : None, area : None, description : None, fulldescription : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,211 [INFO] Title : None, price : None, area : None, description : None, fulldescription : None\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,219 [INFO] Tags : [None]\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,219 [INFO] Tags : [None]\u001b[0m\n",
      "\u001b[31m2025-09-28 16:50:58,225 [ERROR] something failed when scraping the data\u001b[0m\n",
      "\u001b[31m2025-09-28 16:50:58,225 [ERROR] something failed when scraping the data\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,232 [INFO] Trying to get the container element\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,232 [INFO] Trying to get the container element\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,248 [INFO] Trying to get the container's listings\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,248 [INFO] Trying to get the container's listings\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,261 [INFO] Iterating over the container's listings\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,261 [INFO] Iterating over the container's listings\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,262 [INFO] Trying to scrape a listing from the container\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:58,262 [INFO] Trying to scrape a listing from the container\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "containers; 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:50:59,576 [INFO] Acquired listing id 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:59,576 [INFO] Acquired listing id 2342470\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:59,656 [INFO] Title : Studyo, price : From\n",
      "€850 /month, area : 16 m²+, description : Cleaning in common areas, fulldescription : BOOK NOW & get the December rent for free!*\n",
      "\n",
      "Find your preferred room, book online before 30th September and grab one month rent free\n",
      "in December.\n",
      "Spend your December budget on gifts and fun instead of rent.\n",
      "\n",
      "With us you’ll find more than just an apartment. You’ll find a place that feels like home from day one, where you can focus on your studies, build lasting\n",
      "friendships, and become part of a vibrant international community. Be part of it.\n",
      "Our apartments are fully furnished with modern, comfor...\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:59,656 [INFO] Title : Studyo, price : From\n",
      "€850 /month, area : 16 m²+, description : Cleaning in common areas, fulldescription : BOOK NOW & get the December rent for free!*\n",
      "\n",
      "Find your preferred room, book online before 30th September and grab one month rent free\n",
      "in December.\n",
      "Spend your December budget on gifts and fun instead of rent.\n",
      "\n",
      "With us you’ll find more than just an apartment. You’ll find a place that feels like home from day one, where you can focus on your studies, build lasting\n",
      "friendships, and become part of a vibrant international community. Be part of it.\n",
      "Our apartments are fully furnished with modern, comfor...\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:59,665 [INFO] Tags : [None]\u001b[0m\n",
      "\u001b[32m2025-09-28 16:50:59,665 [INFO] Tags : [None]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "containers; 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mscrape_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mscrape_pages\u001b[39m\u001b[34m(driver)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     48\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTrying to Scrape the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpages_scraped\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mth Page\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[43mscrape_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSuccessfuly scraped the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpages_scraped\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mth Page\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     52\u001b[39m     pages_scraped += \u001b[32m1\u001b[39m        \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mscrape_page\u001b[39m\u001b[34m(driver)\u001b[39m\n\u001b[32m     30\u001b[39m scrape_half_page(driver)\n\u001b[32m     31\u001b[39m driver.execute_script(\u001b[33m\"\u001b[39m\u001b[33mwindow.scrollBy(0, 2000);\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mscrape_half_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mscrape_half_page\u001b[39m\u001b[34m(driver)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     15\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[33mTrying to scrape a listing from the container\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     listing_data = \u001b[43mscrape_listing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlisting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_window\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ElementClickInterceptedException:\n\u001b[32m     18\u001b[39m     logger.error(\u001b[33m'\u001b[39m\u001b[33mFailed to scrape a listing from the container\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 165\u001b[39m, in \u001b[36mscrape_listing\u001b[39m\u001b[34m(listing, driver, original_window)\u001b[39m\n\u001b[32m    162\u001b[39m tags = get_listing_tags(driver)\n\u001b[32m    163\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTags : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtags\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m \u001b[43mscrape_listing_photos_and_create_their_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlisting_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m logger.info(\u001b[33m'\u001b[39m\u001b[33mScraped photos\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    168\u001b[39m latitude, longitude, number_of_rooms = get_geo_data(driver)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mscrape_listing_photos_and_create_their_file\u001b[39m\u001b[34m(driver, listing_id)\u001b[39m\n\u001b[32m     68\u001b[39m image_url = image.get_attribute(\u001b[33m'\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m image.get_attribute(\u001b[33m'\u001b[39m\u001b[33mdata-src\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m image.get_attribute(\u001b[33m'\u001b[39m\u001b[33mdata-lazy\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m image.get_attribute(\u001b[33m'\u001b[39m\u001b[33mdata-original\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image_url.startswith(\u001b[33m\"\u001b[39m\u001b[33mhttp\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     image_to_be_written = response.content\n\u001b[32m     75\u001b[39m     image_bytes = \u001b[38;5;28mlen\u001b[39m(image_to_be_written)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\urllib3\\connection.py:741\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m     \u001b[38;5;66;03m# Remove trailing '.' from fqdn hostnames to allow certificate validation\u001b[39;00m\n\u001b[32m    739\u001b[39m     server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m     sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m        \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m        \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    759\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n\u001b[32m    761\u001b[39m \u001b[38;5;66;03m# If an error occurs during connection/handshake we may need to release\u001b[39;00m\n\u001b[32m    762\u001b[39m \u001b[38;5;66;03m# our lock so another connection can probe the origin.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\urllib3\\connection.py:920\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    917\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_ipaddress(normalized):\n\u001b[32m    918\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m920\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m assert_fingerprint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\urllib3\\util\\ssl_.py:460\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    456\u001b[39m         context.load_cert_chain(certfile, keyfile, key_password)\n\u001b[32m    458\u001b[39m context.set_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m ssl_sock = \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\steve\\web-scraping\\.venv\\Lib\\site-packages\\urllib3\\util\\ssl_.py:504\u001b[39m, in \u001b[36m_ssl_wrap_socket_impl\u001b[39m\u001b[34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[39m\n\u001b[32m    501\u001b[39m     SSLTransport._validate_ssl_context_for_tls_in_tls(ssl_context)\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\ssl.py:1042\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1039\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m:\n\u001b[32m   1040\u001b[39m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[32m   1041\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1042\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\ssl.py:1320\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[32m   1319\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.settimeout(timeout)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "scrape_pages(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed697776",
   "metadata": {},
   "source": [
    "Next feature testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def go_to_next_page(driver):\n",
    "    logger.info('Trying to go to the next Page')\n",
    "    next_button = driver.find_element(By.CSS_SELECTOR, \"button[aria-label='Go to next page']\")\n",
    "    next_button.click()\n",
    "    logger.info('Clicked on next Page')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a7b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "driver = get_driver()\n",
    "driver.get('https://housinganywhere.com')\n",
    "search_for_place(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d977b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-28 16:39:07,042 [INFO] Trying to go to the next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,042 [INFO] Trying to go to the next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,042 [INFO] Trying to go to the next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,042 [INFO] Trying to go to the next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,093 [INFO] Clicked on next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,093 [INFO] Clicked on next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,093 [INFO] Clicked on next Page\u001b[0m\n",
      "\u001b[32m2025-09-28 16:39:07,093 [INFO] Clicked on next Page\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# driver.execute_script(\"window.scrollBy(0, 4000);\")\n",
    "# time.sleep(2)\n",
    "# go_to_next_page(driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c134222",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(200, document.body.scrollHeight);\")\n",
    "\n",
    "#go_to_next_page(driver)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
